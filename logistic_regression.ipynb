{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "400c51ff-93f0-4b62-9108-77f32da9118f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOGISTIC REGRESSION CLASSIFIER \n",
    "# Pawan Harikrishnan & Parinita Mithepati \n",
    "\n",
    "# Description:\n",
    "# This program implements the logisitic regression model to perform binary classification on the KDD90 data set \n",
    "# Logistic Regression uses the logistic function to map the prediction between two binary values 0 and 1 \n",
    "# This program uses the scikit logistic regression model to classify data as either normal or not normal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7a4cab5-2224-4624-a340-0826d577f5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1 - Import the training and test files for the dataset \n",
    "import pandas as pd \n",
    "# training data import\n",
    "training_data = pd.read_csv('train_kdd_small.csv')\n",
    "# testing data import\n",
    "testing_data = pd.read_csv('test_kdd_small.csv')\n",
    "# testing and training data do not have the same the distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebf6c59b-8db1-42f3-8c1f-52a1600ae1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the files to make sure theyre being imported correctly \n",
    "#print(training_data)\n",
    "#print(testing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "962d165c-7bf2-43c9-b273-59baa0ceb618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2 - Dataset needs to be modified as not all the columns have numbers as data type \n",
    "# Protocol Type - icmp, tcp \n",
    "# Service - ecr_i, http \n",
    "# Flag - SF\n",
    "# Label - not_normal, normal \n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "\n",
    "# 1.) encode protocol type where icmp = 0, tcp = 1\n",
    "encoder_string_converted_label = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=2,max_categories=3,dtype=int)\n",
    "#new_protocol_maps = [['icmp', 0], ['tcp', 1]]\n",
    "encoder_string_converted_label.fit(training_data[['protocol_type']])\n",
    "# map to the testing and training data\n",
    "training_data['protocol_type'] = encoder_string_converted_label.transform(training_data[['protocol_type']])\n",
    "testing_data['protocol_type'] = encoder_string_converted_label.transform(testing_data[['protocol_type']])\n",
    "# check the data to make sure its getting converted \n",
    "#print(training_data['protocol_type'])\n",
    "#print(testing_data['protocol_type'])\n",
    "\n",
    "# 2.) encode service type where ecr_i = 0, http = 1\n",
    "encoder_string_converted_label = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=2,max_categories=3,dtype=int)\n",
    "#new_protocol_maps = [['ecr_i', 0], ['http', 1]]\n",
    "encoder_string_converted_label.fit(training_data[['service']])\n",
    "# map to the testing and training data\n",
    "training_data['service'] = encoder_string_converted_label.transform(training_data[['service']])\n",
    "testing_data['service'] = encoder_string_converted_label.transform(testing_data[['service']])\n",
    "# check the data to make sure its getting converted \n",
    "#print(training_data['service'])\n",
    "#print(testing_data['service'])\n",
    "\n",
    "# 3.) encode flag type where SF = 0\n",
    "encoder_string_converted_label = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=1,max_categories=2,dtype=int)\n",
    "#new_protocol_maps = [['SF', 0], ['everything else', 1]]\n",
    "encoder_string_converted_label.fit(training_data[['flag']])\n",
    "# map to the testing and training data\n",
    "training_data['flag'] = encoder_string_converted_label.transform(training_data[['flag']])\n",
    "testing_data['flag'] = encoder_string_converted_label.transform(testing_data[['flag']])\n",
    "# check the data to make sure its getting converted \n",
    "#print(training_data['flag'])\n",
    "#print(testing_data['flag'])\n",
    "\n",
    "# 4.) enode label type where not_normal = 0, normal = 1\n",
    "encoder_string_converted_label = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=2,max_categories=3,dtype=int)\n",
    "#new_protocol_maps = [['normal', 1], ['not_normal', 0]]\n",
    "encoder_string_converted_label.fit(training_data[['label']])\n",
    "# map to the testing and training data\n",
    "training_data['label'] = encoder_string_converted_label.transform(training_data[['label']])\n",
    "testing_data['label'] = encoder_string_converted_label.transform(testing_data[['label']])\n",
    "# check the data to make sure its getting converted \n",
    "#print(training_data['label'])\n",
    "#print(testing_data['label'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3af7fa00-3fd8-4be3-8775-67cb86b41dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original training feature shape:  (2799, 42)\n",
      "New training feature shape:  (2799, 41)\n",
      "New final feature shape:  (2799,)\n",
      "Original testing feature shape:  (1199, 42)\n",
      "New testing feature shape:  (1199, 41)\n",
      "New final feature shape:  (1199,)\n",
      "label\n",
      "0    600\n",
      "1    599\n",
      "Name: count, dtype: int64\n",
      "label\n",
      "1    1401\n",
      "0    1398\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# STEP 3 - The model now needs to be trained using the training set without the labels\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# this should be everything in the training set\n",
    "print(\"Original training feature shape: \",training_data.shape)\n",
    "# create the final training set which is everything except the label column feature\n",
    "x_training_final = training_data.drop(columns=['label'])\n",
    "# this should be everything except the last column \n",
    "print(\"New training feature shape: \",x_training_final.shape)\n",
    "\n",
    "\n",
    "# create the final label feature column for the training set\n",
    "y_training_final = training_data['label']\n",
    "# this should be just the last column \n",
    "print(\"New final feature shape: \",y_training_final.shape)\n",
    "\n",
    "\n",
    "# this should be everything in the test set\n",
    "print(\"Original testing feature shape: \",testing_data.shape)\n",
    "# create the final training set which is everything except the label column feature\n",
    "x_testing_final = testing_data.drop(columns=['label'])\n",
    "# this should be everything except the last column \n",
    "print(\"New testing feature shape: \",x_testing_final.shape)\n",
    "\n",
    "\n",
    "# create the final label feature column for the testing set\n",
    "y_testing_final = testing_data['label']\n",
    "# this should be just the last column \n",
    "print(\"New final feature shape: \",y_testing_final.shape)\n",
    "\n",
    "print(y_testing_final.value_counts())\n",
    "print(y_training_final.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07a36d6a-1c4c-4c9d-8d77-bf55fbafc0ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8410214463203252\n",
      "0.8562703794365475\n"
     ]
    }
   ],
   "source": [
    "# STEP 4 (OPTIMIZATION STEP) - Dataset needs to be normalized as some features range from 0-1 and some from 300-1000 \n",
    "# logisitic regression uses the sigmoid function to classifiy data as either 0 or 1\n",
    "# when the data range is too big like in src_bytes the data can range from 1032 to 201 \n",
    "# so this data has to be normalized, normalization will subtract the value from the mean so it will be on a scale from 0 to 1 \n",
    "# this will make sure that the sigmoid function does not lean towards 1 for values that are too big and 0 for values that are too small \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# create the scaler\n",
    "scaler_logistic_regression = StandardScaler()\n",
    "\n",
    "# normalize Xtrain and Xtest from the last step so some features are not too skewed, need to do fit_transform first or it;ll throw an error\n",
    "Xtrain_normalized = scaler_logistic_regression.fit_transform(x_training_final)\n",
    "Xtest_normalized  = scaler_logistic_regression.transform(x_testing_final)\n",
    "\n",
    "# Standard deviation needs to be about 1, this proved that thje data was normalized \n",
    "print(Xtrain_normalized.std())\n",
    "print(Xtest_normalized.std())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ac87afa-4d01-4ecb-9330-87ef56a8cd88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 1.0\n",
      "Precision: 1.0\n",
      "Recall   : 1.0\n",
      "F1 Score : 1.0\n"
     ]
    }
   ],
   "source": [
    "# STEP 5 - Create and run the model on the modified dataset with the normalized values \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# set the model type to be the logistic regression model \n",
    "model = LogisticRegression(penalty='l2', solver='liblinear', max_iter=2000, random_state=42) \n",
    "\n",
    "# fit the model using the normalized training and testing created above\n",
    "model.fit(Xtrain_normalized,y_training_final)\n",
    "\n",
    "\n",
    "# FINAL MODEL PREDICTIONS \n",
    "y_pred = model.predict(Xtest_normalized)\n",
    "\n",
    "\n",
    "# FINAL SCORES \n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "print(\"Accuracy :\", accuracy_score(y_testing_final, y_pred))\n",
    "print(\"Precision:\", precision_score(y_testing_final, y_pred))\n",
    "print(\"Recall   :\", recall_score(y_testing_final, y_pred))\n",
    "print(\"F1 Score :\", f1_score(y_testing_final, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d6d24d-eda2-43a7-af9a-64ac9cb31ebc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189c215c-a0ac-4e89-9e05-9912fb9cfaab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "classic_machine_learning",
   "language": "python",
   "name": "miniproject2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
