{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b571e9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART 1\n",
    "# ------------------------------------------------------------------------------------------------------------------------\n",
    "# LOGISTIC REGRESSION CLASSIFIER \n",
    "# Pawan Harikrishnan & Parinita Mithepati \n",
    "\n",
    "# Description:\n",
    "# This program implements the logisitic regression model to perform binary classification on the KDD90 data set \n",
    "# Logistic Regression uses the logistic function to map the prediction between two binary values 0 and 1 \n",
    "# This program uses the scikit logistic regression model to classify data as either normal or not normal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8126a5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1 - Import the training and test files for the dataset \n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "# training data import\n",
    "training_data = pd.read_csv('train_kdd_small.csv')\n",
    "# testing data import\n",
    "testing_data = pd.read_csv('test_kdd_small.csv')\n",
    "# testing and training data do not have the same the distribution\n",
    "\n",
    "# check the files to make sure theyre being imported correctly \n",
    "#print(training_data)\n",
    "#print(testing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb32aa7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2 - Dataset needs to be modified as not all the columns have numbers as data type \n",
    "# Protocol Type - icmp, tcp \n",
    "# Service - ecr_i, http \n",
    "# Flag - SF\n",
    "# Label - not_normal, normal \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# create the label encoder \n",
    "encoder_string_converted_label = LabelEncoder()\n",
    "\n",
    "# 1.) encode protocol type where icmp = 0, tcp = 1\n",
    "encoder_string_converted_label.fit(training_data['protocol_type'])\n",
    "# map to the testing and training data\n",
    "training_data['protocol_type'] = encoder_string_converted_label.transform(training_data['protocol_type'])\n",
    "testing_data['protocol_type'] = encoder_string_converted_label.transform(testing_data['protocol_type'])\n",
    "# check the data to make sure its getting converted \n",
    "#print(training_data['protocol_type'])\n",
    "#print(testing_data['protocol_type'])\n",
    "\n",
    "# 2.) encode servicve type where ecr_i = 0, http = 1\n",
    "encoder_string_converted_label.fit(training_data['service'])\n",
    "# map to the testing and training data\n",
    "training_data['service'] = encoder_string_converted_label.transform(training_data['service'])\n",
    "testing_data['service'] = encoder_string_converted_label.transform(testing_data['service'])\n",
    "# check the data to make sure its getting converted \n",
    "#print(training_data['service'])\n",
    "#print(testing_data['service'])\n",
    "\n",
    "# 3.) encode flag type where SF=0\n",
    "encoder_string_converted_label.fit(training_data['flag'])\n",
    "# map to the testing and training data\n",
    "training_data['flag'] = encoder_string_converted_label.transform(training_data['flag'])\n",
    "testing_data['flag'] = encoder_string_converted_label.transform(testing_data['flag'])\n",
    "# check the data to make sure its getting converted \n",
    "#print(training_data['flag'])\n",
    "#print(testing_data['flag'])\n",
    "\n",
    "\n",
    "# 4.) encode label type where not_normal = 0, normal = 1\n",
    "encoder_string_converted_label.fit(training_data['label'])\n",
    "# map to the testing and training data\n",
    "training_data['label'] = encoder_string_converted_label.transform(training_data['label'])\n",
    "testing_data['label'] = encoder_string_converted_label.transform(testing_data['label'])\n",
    "# check the data to make sure its getting converted \n",
    "#print(training_data['label'])\n",
    "#print(testing_data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25a233ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 3 - The model now needs to be trained using the training set without the labels\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# NEW TRAINING FILE\n",
    "x_training_dropped = training_data.drop('label',axis=1)\n",
    "y_training_final = training_data['label']\n",
    "# check to make sure it dropped the last column\n",
    "#print(x_training_final.shape)\n",
    "#print(x_training_final.head())\n",
    "#print(y_training_final.head())\n",
    "\n",
    "# NEW TESTING FILE\n",
    "x_testing_dropped = testing_data.drop('label',axis=1)\n",
    "y_testing_final = testing_data['label']\n",
    "# check to make sure it dropped the last column\n",
    "#print(x_testing_final.shape)\n",
    "#print(x_testing_final.head())\n",
    "#print(y_testing_final.head())\n",
    "\n",
    "# VALIDATION SETS\n",
    "x_training_set, x_validation_set, y_training_set, y_validation_set = train_test_split(x_training_dropped, y_training_final,test_size=0.20,random_state=42,stratify=y_training_final)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "443ebbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 4 (OPTIMIZATION STEP) \n",
    "# currently the model is too good, so its probably running into overfitting issues \n",
    "# this is the model output with no optimization steps\n",
    "\n",
    "# MODEL OUTPUT\n",
    "# Model Accuracy : % 100.0\n",
    "# Model Precision: % 100.0\n",
    "# Model Recall   : % 100.0\n",
    "# Model F1 Score : % 100.0\n",
    "\n",
    "# 1.) add the l2 regularzation penalty  \n",
    "\n",
    "# 2.) choose a really low value for C \n",
    "\n",
    "# 3.) add noice to the training data  (skipping for now)\n",
    "\n",
    "# 4.) normalize the data \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# create the scalar first\n",
    "scaler_logistic_regression = StandardScaler()\n",
    "\n",
    "\n",
    "# normalize x_training_final and x_testing_final from the last step so some features are not too skewed, need to do fit_transform first or it;ll throw an error\n",
    "x_training_normalized = scaler_logistic_regression.fit_transform(x_training_set)\n",
    "x_validation_normalized = scaler_logistic_regression.transform(x_validation_set)\n",
    "x_testing_normalized  = scaler_logistic_regression.transform(x_testing_dropped)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2940924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Validation Metrics: \n",
      "Model Accuracy : % 99.28571428571429\n",
      "Model Precision: % 100.0\n",
      "Model Recall   : % 98.57142857142858\n",
      "Model F1 Score : % 99.28057553956835\n",
      "The validation error is: % 0.7142857142857117\n",
      "Final Model Metrics: \n",
      "Model Accuracy : % 99.58298582151794\n",
      "Model Precision: % 100.0\n",
      "Model Recall   : % 99.1652754590985\n",
      "Model F1 Score : % 99.58088851634534\n",
      "The testing error is: % 0.4170141784820669\n"
     ]
    }
   ],
   "source": [
    "# STEP 5 - Create and run the model on the modified dataset with the normalized values \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# set the model type to be the logistic regression model \n",
    "model = LogisticRegression(penalty='l2',C=0.0001, solver='liblinear',random_state=42) \n",
    "\n",
    "# fit the model using the normalized training and testing created above\n",
    "model.fit(x_training_normalized,y_training_set)\n",
    "\n",
    "# FINAL VALIDATION PREDICTIONS\n",
    "y_final_validation_prediction = model.predict(x_validation_normalized)\n",
    "\n",
    "# FINAL MODEL PREDICTIONS \n",
    "y_final_prediction = model.predict(x_testing_normalized)\n",
    "\n",
    "\n",
    "# FINAL SCORES \n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "# Validation scores\n",
    "model_validation_accuracy = accuracy_score(y_validation_set, y_final_validation_prediction)\n",
    "model_validation_precision = precision_score(y_validation_set, y_final_validation_prediction)\n",
    "model_validation_recall = recall_score(y_validation_set, y_final_validation_prediction)\n",
    "model_validation_f1_score = f1_score(y_validation_set, y_final_validation_prediction)\n",
    "\n",
    "# Test scores \n",
    "model_accuracy = accuracy_score(y_testing_final, y_final_prediction)\n",
    "model_precision = precision_score(y_testing_final, y_final_prediction)\n",
    "model_recall = recall_score(y_testing_final, y_final_prediction)\n",
    "model_f1_score = f1_score(y_testing_final, y_final_prediction)\n",
    "\n",
    "print(\"Final Validation Metrics: \")\n",
    "\n",
    "print(\"Model Accuracy : %\", model_validation_accuracy*100)\n",
    "print(\"Model Precision: %\", model_validation_precision*100)\n",
    "print(\"Model Recall   : %\", model_validation_recall*100)\n",
    "print(\"Model F1 Score : %\", model_validation_f1_score*100)\n",
    "\n",
    "print(\"The validation error is: %\", (1-model_validation_accuracy)*100)\n",
    "\n",
    "print(\"Final Model Metrics: \")\n",
    "\n",
    "print(\"Model Accuracy : %\", model_accuracy*100)\n",
    "print(\"Model Precision: %\", model_precision*100)\n",
    "print(\"Model Recall   : %\", model_recall*100)\n",
    "print(\"Model F1 Score : %\", model_f1_score*100)\n",
    "\n",
    "print(\"The testing error is: %\", (1-model_accuracy)*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e914a7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM CLASSIFIER \n",
    "# Pawan Harikrishnan & Parinita Mithepati \n",
    "\n",
    "# Description:\n",
    "# This program implements the SVM (Support Vector Machine) model to perform binary classification on the KDD90 data set \n",
    "# SVM will find a split on the plane containing the data, so it find a plane that splits the data into two parts \n",
    "# This program uses the scikit SVM model to classify data as either normal or not normal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "142f84fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1 - Import the training and test files for the dataset \n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "# training data import\n",
    "training_data = pd.read_csv('train_kdd_small.csv')\n",
    "# testing data import\n",
    "testing_data = pd.read_csv('test_kdd_small.csv')\n",
    "# testing and training data do not have the same the distribution\n",
    "\n",
    "# check the files to make sure theyre being imported correctly \n",
    "#print(training_data)\n",
    "#print(testing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5bb7d827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2 - Dataset needs to be modified as not all the columns have numbers as data type \n",
    "# Protocol Type - icmp, tcp \n",
    "# Service - ecr_i, http \n",
    "# Flag - SF\n",
    "# Label - not_normal, normal \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# create the label encoder \n",
    "encoder_string_converted_label = LabelEncoder()\n",
    "\n",
    "# 1.) encode protocol type where icmp = 0, tcp = 1\n",
    "encoder_string_converted_label.fit(training_data['protocol_type'])\n",
    "# map to the testing and training data\n",
    "training_data['protocol_type'] = encoder_string_converted_label.transform(training_data['protocol_type'])\n",
    "testing_data['protocol_type'] = encoder_string_converted_label.transform(testing_data['protocol_type'])\n",
    "# check the data to make sure its getting converted \n",
    "#print(training_data['protocol_type'])\n",
    "#print(testing_data['protocol_type'])\n",
    "\n",
    "# 2.) encode servicve type where ecr_i = 0, http = 1\n",
    "encoder_string_converted_label.fit(training_data['service'])\n",
    "# map to the testing and training data\n",
    "training_data['service'] = encoder_string_converted_label.transform(training_data['service'])\n",
    "testing_data['service'] = encoder_string_converted_label.transform(testing_data['service'])\n",
    "# check the data to make sure its getting converted \n",
    "#print(training_data['service'])\n",
    "#print(testing_data['service'])\n",
    "\n",
    "# 3.) encode flag type where SF=0\n",
    "encoder_string_converted_label.fit(training_data['flag'])\n",
    "# map to the testing and training data\n",
    "training_data['flag'] = encoder_string_converted_label.transform(training_data['flag'])\n",
    "testing_data['flag'] = encoder_string_converted_label.transform(testing_data['flag'])\n",
    "# check the data to make sure its getting converted \n",
    "#print(training_data['flag'])\n",
    "#print(testing_data['flag'])\n",
    "\n",
    "\n",
    "# 4.) encode label type where not_normal = 0, normal = 1\n",
    "encoder_string_converted_label.fit(training_data['label'])\n",
    "# map to the testing and training data\n",
    "training_data['label'] = encoder_string_converted_label.transform(training_data['label'])\n",
    "testing_data['label'] = encoder_string_converted_label.transform(testing_data['label'])\n",
    "# check the data to make sure its getting converted \n",
    "#print(training_data['label'])\n",
    "#print(testing_data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9717de84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 3 - The model now needs to be trained using the training set without the labels\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# NEW TRAINING FILE\n",
    "x_training_final = training_data.drop('label',axis=1)\n",
    "y_training_final = training_data['label']\n",
    "# check to make sure it dropped the last column\n",
    "#print(x_training_final.shape)\n",
    "#print(x_training_final.head())\n",
    "#print(y_training_final.head())\n",
    "\n",
    "# NEW TESTING FILE\n",
    "x_testing_final = testing_data.drop('label',axis=1)\n",
    "y_testing_final = testing_data['label']\n",
    "# check to make sure it dropped the last column\n",
    "#print(x_testing_final.shape)\n",
    "#print(x_testing_final.head())\n",
    "#print(y_testing_final.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f62f4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 4 (OPTIMIZATION STEP) \n",
    "# currently the model is too good, so its probably running into overfitting issues \n",
    "# this is the model output with no optimization steps\n",
    "\n",
    "# MODEL OUTPUT\n",
    "# Model Accuracy : % 100.0\n",
    "# Model Precision: % 100.0\n",
    "# Model Recall   : % 100.0\n",
    "# Model F1 Score : % 100.0\n",
    "\n",
    "# 1.) add the l2 regularzation penalty  \n",
    "\n",
    "# 2.) choose a really low value for C \n",
    "\n",
    "# 3.) add noice to the training data  (skipping for now)\n",
    "\n",
    "# 4.) normalize the data \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# create the scalar first\n",
    "scaler_svm = StandardScaler()\n",
    "\n",
    "\n",
    "# normalize x_training_final and x_testing_final from the last step so some features are not too skewed, need to do fit_transform first or it;ll throw an error\n",
    "x_training_normalized = scaler_svm.fit_transform(x_training_final)\n",
    "x_testing_normalized  = scaler_svm.transform(x_testing_final)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e1c9099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Model Metrics: \n",
      "Model Accuracy : % 99.83319432860718\n",
      "Model Precision: % 100.0\n",
      "Model Recall   : % 99.6661101836394\n",
      "Model F1 Score : % 99.83277591973244\n"
     ]
    }
   ],
   "source": [
    "# STEP 5 - Create and run the model on the modified dataset with the normalized values \n",
    "from sklearn import svm\n",
    "# set the model type to be the SVM model\n",
    "model = svm.SVC(kernel='rbf', C=0.1)\n",
    "\n",
    "# fit the model using the normalized training and testing created above\n",
    "model.fit(x_training_normalized,y_training_final)\n",
    "\n",
    "# FINAL MODEL PREDICTIONS \n",
    "y_final_prediction = model.predict(x_testing_normalized)\n",
    "\n",
    "\n",
    "# FINAL SCORES \n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "model_accuracy = accuracy_score(y_testing_final, y_final_prediction)\n",
    "model_precision = precision_score(y_testing_final, y_final_prediction)\n",
    "model_recall = recall_score(y_testing_final, y_final_prediction)\n",
    "model_f1_score = f1_score(y_testing_final, y_final_prediction)\\\n",
    "\n",
    "print(\"Final Model Metrics: \")\n",
    "\n",
    "print(\"Model Accuracy : %\", model_accuracy*100)\n",
    "print(\"Model Precision: %\", model_precision*100)\n",
    "print(\"Model Recall   : %\", model_recall*100)\n",
    "print(\"Model F1 Score : %\", model_f1_score*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "927337e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forests CLASSIFIER \n",
    "# Pawan Harikrishnan & Parinita Mithepati \n",
    "\n",
    "# Description:\n",
    "# This program implements the Random Forests model to perform binary classification on the KDD90 data set \n",
    "# Random Forests use decision tress to keep splitting on a feature to classify data\n",
    "# This program uses the scikit random forests model to classify data as either normal or not normal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "92884302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1 - Import the training and test files for the dataset \n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "# training data import\n",
    "training_data = pd.read_csv('train_kdd_small.csv')\n",
    "# testing data import\n",
    "testing_data = pd.read_csv('test_kdd_small.csv')\n",
    "# testing and training data do not have the same the distribution\n",
    "\n",
    "# check the files to make sure theyre being imported correctly \n",
    "#print(training_data)\n",
    "#print(testing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "50f73482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2 - Dataset needs to be modified as not all the columns have numbers as data type \n",
    "# Protocol Type - icmp, tcp \n",
    "# Service - ecr_i, http \n",
    "# Flag - SF\n",
    "# Label - not_normal, normal \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# create the label encoder \n",
    "encoder_string_converted_label = LabelEncoder()\n",
    "\n",
    "# 1.) encode protocol type where icmp = 0, tcp = 1\n",
    "encoder_string_converted_label.fit(training_data['protocol_type'])\n",
    "# map to the testing and training data\n",
    "training_data['protocol_type'] = encoder_string_converted_label.transform(training_data['protocol_type'])\n",
    "testing_data['protocol_type'] = encoder_string_converted_label.transform(testing_data['protocol_type'])\n",
    "# check the data to make sure its getting converted \n",
    "#print(training_data['protocol_type'])\n",
    "#print(testing_data['protocol_type'])\n",
    "\n",
    "# 2.) encode servicve type where ecr_i = 0, http = 1\n",
    "encoder_string_converted_label.fit(training_data['service'])\n",
    "# map to the testing and training data\n",
    "training_data['service'] = encoder_string_converted_label.transform(training_data['service'])\n",
    "testing_data['service'] = encoder_string_converted_label.transform(testing_data['service'])\n",
    "# check the data to make sure its getting converted \n",
    "#print(training_data['service'])\n",
    "#print(testing_data['service'])\n",
    "\n",
    "# 3.) encode flag type where SF=0\n",
    "encoder_string_converted_label.fit(training_data['flag'])\n",
    "# map to the testing and training data\n",
    "training_data['flag'] = encoder_string_converted_label.transform(training_data['flag'])\n",
    "testing_data['flag'] = encoder_string_converted_label.transform(testing_data['flag'])\n",
    "# check the data to make sure its getting converted \n",
    "#print(training_data['flag'])\n",
    "#print(testing_data['flag'])\n",
    "\n",
    "\n",
    "# 4.) encode label type where not_normal = 0, normal = 1\n",
    "encoder_string_converted_label.fit(training_data['label'])\n",
    "# map to the testing and training data\n",
    "training_data['label'] = encoder_string_converted_label.transform(training_data['label'])\n",
    "testing_data['label'] = encoder_string_converted_label.transform(testing_data['label'])\n",
    "# check the data to make sure its getting converted \n",
    "#print(training_data['label'])\n",
    "#print(testing_data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "10425c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 3 - The model now needs to be trained using the training set without the labels\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# NEW TRAINING FILE\n",
    "x_training_final = training_data.drop('label',axis=1)\n",
    "y_training_final = training_data['label']\n",
    "# check to make sure it dropped the last column\n",
    "#print(x_training_final.shape)\n",
    "#print(x_training_final.head())\n",
    "#print(y_training_final.head())\n",
    "\n",
    "# NEW TESTING FILE\n",
    "x_testing_final = testing_data.drop('label',axis=1)\n",
    "y_testing_final = testing_data['label']\n",
    "# check to make sure it dropped the last column\n",
    "#print(x_testing_final.shape)\n",
    "#print(x_testing_final.head())\n",
    "#print(y_testing_final.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a2eca6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 4 (OPTIMIZATION STEP) \n",
    "# currently the model is too good, so its probably running into overfitting issues \n",
    "# this is the model output with no optimization steps\n",
    "\n",
    "# MODEL OUTPUT\n",
    "# Model Accuracy : % 100.0\n",
    "# Model Precision: % 100.0\n",
    "# Model Recall   : % 100.0\n",
    "# Model F1 Score : % 100.0\n",
    "\n",
    "# 1.) add the l2 regularzation penalty  \n",
    "\n",
    "# 2.) choose a really low value for C \n",
    "\n",
    "# 3.) add noice to the training data  (skipping for now)\n",
    "\n",
    "# 4.) normalize the data \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# create the scalar first\n",
    "scaler_random_forests = StandardScaler()\n",
    "\n",
    "\n",
    "# normalize x_training_final and x_testing_final from the last step so some features are not too skewed, need to do fit_transform first or it;ll throw an error\n",
    "x_training_normalized = scaler_random_forests.fit_transform(x_training_final)\n",
    "x_testing_normalized  = scaler_random_forests.transform(x_testing_final)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0cd06d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Model Metrics: \n",
      "Model Accuracy : % 99.74979149291076\n",
      "Model Precision: % 100.0\n",
      "Model Recall   : % 99.4991652754591\n",
      "Model F1 Score : % 99.7489539748954\n"
     ]
    }
   ],
   "source": [
    "# STEP 5 - Create and run the model on the modified dataset with the normalized values \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# set the model type to be the SVM model\n",
    "model = RandomForestClassifier(max_depth=100,random_state=42,n_estimators=1000,min_samples_leaf=900)\n",
    "\n",
    "# fit the model using the normalized training and testing created above\n",
    "model.fit(x_training_normalized,y_training_final)\n",
    "\n",
    "# FINAL MODEL PREDICTIONS \n",
    "y_final_prediction = model.predict(x_testing_normalized)\n",
    "\n",
    "\n",
    "# FINAL SCORES \n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "model_accuracy = accuracy_score(y_testing_final, y_final_prediction)\n",
    "model_precision = precision_score(y_testing_final, y_final_prediction)\n",
    "model_recall = recall_score(y_testing_final, y_final_prediction)\n",
    "model_f1_score = f1_score(y_testing_final, y_final_prediction)\\\n",
    "\n",
    "print(\"Final Model Metrics: \")\n",
    "\n",
    "print(\"Model Accuracy : %\", model_accuracy*100)\n",
    "print(\"Model Precision: %\", model_precision*100)\n",
    "print(\"Model Recall   : %\", model_recall*100)\n",
    "print(\"Model F1 Score : %\", model_f1_score*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5310e5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART 2 \n",
    "# ---------------------------------------------------------------------------------------------------------------------------\n",
    "# NEURAL NET\n",
    "# Pawan Harikrishnan & Parinita Mithepati "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3b50d8fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
      "Requirement already satisfied: torch in c:\\users\\pawan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.5.1+cu118)\n",
      "Requirement already satisfied: torchvision in c:\\users\\pawan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.20.1)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\pawan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.5.1+cu118)\n",
      "Requirement already satisfied: fsspec in c:\\users\\pawan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\pawan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (3.7.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\pawan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\pawan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\pawan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\pawan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\pawan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\pawan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torchvision) (2.1.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\pawan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\pawan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->torch) (2.1.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 25.0.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\pawan\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "# STEP 0 - Install pytorch and its dependencies\n",
    "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5c03d02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "be9ebe7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1 - Import the training and test files for the dataset \n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "# training data import\n",
    "training_data = pd.read_csv('train_kdd_small.csv')\n",
    "# testing data import\n",
    "testing_data = pd.read_csv('test_kdd_small.csv')\n",
    "# testing and training data do not have the same the distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "26c60836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2 - Dataset needs to be modified as not all the columns have numbers as data type \n",
    "# Protocol Type - icmp, tcp \n",
    "# Service - ecr_i, http \n",
    "# Flag - SF\n",
    "# Label - not_normal, normal \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# create the label encoder \n",
    "encoder_string_converted_label = LabelEncoder()\n",
    "\n",
    "# 1.) encode protocol type where icmp = 0, tcp = 1\n",
    "encoder_string_converted_label.fit(training_data['protocol_type'])\n",
    "# map to the testing and training data\n",
    "training_data['protocol_type'] = encoder_string_converted_label.transform(training_data['protocol_type'])\n",
    "testing_data['protocol_type'] = encoder_string_converted_label.transform(testing_data['protocol_type'])\n",
    "# check the data to make sure its getting converted \n",
    "#print(training_data['protocol_type'])\n",
    "#print(testing_data['protocol_type'])\n",
    "\n",
    "# 2.) encode servicve type where ecr_i = 0, http = 1\n",
    "encoder_string_converted_label.fit(training_data['service'])\n",
    "# map to the testing and training data\n",
    "training_data['service'] = encoder_string_converted_label.transform(training_data['service'])\n",
    "testing_data['service'] = encoder_string_converted_label.transform(testing_data['service'])\n",
    "# check the data to make sure its getting converted \n",
    "#print(training_data['service'])\n",
    "#print(testing_data['service'])\n",
    "\n",
    "# 3.) encode flag type where SF=0\n",
    "encoder_string_converted_label.fit(training_data['flag'])\n",
    "# map to the testing and training data\n",
    "training_data['flag'] = encoder_string_converted_label.transform(training_data['flag'])\n",
    "testing_data['flag'] = encoder_string_converted_label.transform(testing_data['flag'])\n",
    "# check the data to make sure its getting converted \n",
    "#print(training_data['flag'])\n",
    "#print(testing_data['flag'])\n",
    "\n",
    "\n",
    "# 4.) encode label type where not_normal = 0, normal= 1\n",
    "encoder_string_converted_label.fit(training_data['label'])\n",
    "# map to the testing and training data\n",
    "training_data['label'] = encoder_string_converted_label.transform(training_data['label'])\n",
    "testing_data['label'] = encoder_string_converted_label.transform(testing_data['label'])\n",
    "# check the data to make sure its getting converted \n",
    "#print(training_data['label'])\n",
    "#print(testing_data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d6189b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 3 - The model now needs to be trained using the training set without the labels\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# NEW TRAINING FILE\n",
    "x_training_dropped = training_data.drop('label',axis=1)\n",
    "y_training_final = training_data['label']\n",
    "# check to make sure it dropped the last column\n",
    "#print(x_training_final.shape)\n",
    "#print(x_training_final.head())\n",
    "#print(y_training_final.head())\n",
    "\n",
    "# NEW TESTING FILE\n",
    "x_testing_dropped = testing_data.drop('label',axis=1)\n",
    "y_testing_final = testing_data['label']\n",
    "# check to make sure it dropped the last column\n",
    "#print(x_testing_final.shape)\n",
    "#print(x_testing_final.head())\n",
    "#print(y_testing_final.head())\n",
    "\n",
    "# VALIDATION SETS\n",
    "x_training_set, x_validation_set, y_training_set, y_validation_set = train_test_split(x_training_dropped, y_training_final,test_size=0.20,random_state=1,stratify=y_training_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8b00411a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 4 (OPTIMIZATION STEP) \n",
    "# currently the model is too good, so its probably running into overfitting issues \n",
    "# this is the model output with no optimization steps\n",
    "\n",
    "# MODEL OUTPUT\n",
    "# Model Accuracy : % 100.0\n",
    "# Model Precision: % 100.0\n",
    "# Model Recall   : % 100.0\n",
    "# Model F1 Score : % 100.0\n",
    "\n",
    "# 1.) add the l2 regularzation penalty  \n",
    "\n",
    "# 2.) choose a really low value for C \n",
    "\n",
    "# 3.) add noice to the training data  (skipping for now)\n",
    "\n",
    "# 4.) normalize the data \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# create the scalar first\n",
    "scaler_logistic_regression = StandardScaler()\n",
    "\n",
    "\n",
    "# normalize x_training_final and x_testing_final from the last step so some features are not too skewed, need to do fit_transform first or it;ll throw an error\n",
    "x_training_normalized = scaler_logistic_regression.fit_transform(x_training_set)\n",
    "x_validation_normalized = scaler_logistic_regression.transform(x_validation_set)\n",
    "x_testing_normalized  = scaler_logistic_regression.transform(x_testing_dropped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8e796f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 5 - Create the neural net class now \n",
    "# use the neural net module \n",
    "class NeuralNetKDD90(nn.Module):\n",
    "    # start with the init part \n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        # first call is to super to set the values \n",
    "        super(NeuralNetKDD90,self).__init__() \n",
    "        # inputs -> hidden layers + the function to add the bias -> outputs \n",
    "        \n",
    "        # incoming inputs need to get mapped to the first set of hidden layers \n",
    "        self.fc1 = nn.Linear(input_size,hidden_size)\n",
    "        # from these layers the get mapped to the outputs \n",
    "        self.fc2 = nn.Linear(hidden_size,output_size)\n",
    "        # use ReLU to decide if the input -> hidden layer -> output path should be there\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    # now it needs to be forwarded ahead\n",
    "    def forward(self, x):\n",
    "        forwarded_x = x\n",
    "        # output needs to be forwarded along as input --> activation --> output\n",
    "        # so first the weights get multiplied here and the bias is added here\n",
    "        forwarded_x = self.fc1(forwarded_x)\n",
    "        forwarded_x  = self.relu(forwarded_x)\n",
    "        # weights and bias to the last layer before it goes to the output\n",
    "        forward_x = self.fc2(forward_x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebe0bf4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
